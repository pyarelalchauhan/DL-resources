{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewEU9GTxMnAn",
        "outputId": "0fb21d92-d83d-4ae8-8e69-e2100497d8f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights and biases of each layer:\n",
            "Weight of the first layer:\n",
            "torch.Size([5, 3])\n",
            "Bias of the first layer:\n",
            "torch.Size([5])\n",
            "Weight of the second layer:\n",
            "tensor([[ 1.,  1., -1.,  0.,  0.],\n",
            "        [ 0.,  0.,  1.,  1., -1.]])\n",
            "Bias of the second layer:\n",
            "tensor([0., 0.])\n",
            "Network structure:\n",
            "MLP(\n",
            "  (fc1): Linear(in_features=3, out_features=5, bias=True)\n",
            "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
            "  (fc3): Linear(in_features=2, out_features=3, bias=True)\n",
            "  (fc4): Linear(in_features=3, out_features=2, bias=True)\n",
            "  (fc5): Linear(in_features=2, out_features=2, bias=True)\n",
            "  (fc6): Linear(in_features=2, out_features=2, bias=True)\n",
            "  (fc7): Linear(in_features=2, out_features=1, bias=True)\n",
            ")\n",
            "torch.Size([2, 3])\n",
            "# Now let's create the input vector x for a batch of 3 inputs\n",
            "x: tensor([[3., 4., 5.],\n",
            "        [5., 4., 3.]])\n",
            "wx + b for the first layer:\n",
            " tensor([[5., 3.],\n",
            "        [4., 4.],\n",
            "        [3., 5.],\n",
            "        [7., 9.],\n",
            "        [9., 7.]])\n",
            "ReLU applied to wx + b for the first layer:\n",
            " tensor([[5., 3.],\n",
            "        [4., 4.],\n",
            "        [3., 5.],\n",
            "        [7., 9.],\n",
            "        [9., 7.]])\n",
            "wx + b for the second layer:\n",
            " tensor([[6., 2.],\n",
            "        [1., 7.]])\n",
            "ReLU applied to wx + b for the second layer:\n",
            " tensor([[6., 2.],\n",
            "        [1., 7.]])\n",
            "Output after forward pass through the network:\n",
            " tensor([[0., 0.]], grad_fn=<TBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Ex5: Seven Layers aka Multilayered Perceptron (MLP)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define your network architecture\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(3, 5)  # Layer1\n",
        "        self.fc2 = nn.Linear(5, 2)  # layer2\n",
        "        self.fc3 = nn.Linear(2,3)   # Layer3\n",
        "        self.fc4 = nn.Linear(3,2)   # layer4\n",
        "        self.fc5 = nn.Linear(2,2)\n",
        "        self.fc6 = nn.Linear(2,2)\n",
        "        self.fc7 = nn.Linear(2,1)   # output layer\n",
        "\n",
        "        # Manually set weights and biases\n",
        "        self.fc1.weight.data = torch.tensor([[0.0, 0.0, 1.0],   # Weights from input to hidden layer ( 5*3)\n",
        "                                             [0.0, 1.0, 0.0],\n",
        "                                             [1.0, 0.0, 0.0],\n",
        "                                             [1.0, 1.0, 0.0],\n",
        "                                             [0.0, 1.0, 1.0]])\n",
        "        self.fc1.bias.data = torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0])  # Biases for hidden layer\n",
        "\n",
        "        self.fc2.weight.data = torch.tensor([[1.0, 1.0, -1.0, 0.0, 0.0],\n",
        "                                             [0.0, 0.0, 1.0, 1.0, -1.0]])  # weights for the layer 2 ( 2*5)\n",
        "        self.fc2.bias.data = torch.tensor([0.0, 0.0])      # bias for output layer (2*1)\n",
        "        self.fc3.weight.data = torch.tensor([[1.0,1.0],\n",
        "                                            [1.0,-1.0],\n",
        "                                            [1.0,2.0]]) # 3*2\n",
        "        self.fc3.bias.data = torch.tensor([0.0,0.0,0.0]) # 3*1\n",
        "        self.fc4.weight.data = torch.tensor([[1.0,-1.0,0.0],\n",
        "                                            [0.0,-1.0,1.0]]) # 2*3\n",
        "        self.fc4.bias.data = torch.tensor([0.0,0.0]) # 2*1\n",
        "        self.fc5.weight.data = torch.tensor([[0.0,1.0],\n",
        "                                            [1.0,0.0]]) # 2*2\n",
        "        self.fc5.bias.data = torch.tensor([0.0,0.0]) # 2*1\n",
        "        self.fc6.weight.data = torch.tensor([[1.0,-1.0],\n",
        "                                            [1.0,1.0]]) # 2*2\n",
        "        self.fc6.bias.data = torch.tensor([0.0,0.0]) # 2*1\n",
        "        self.fc7.weight.data = torch.tensor([[1.0,-1.0]]) # 1*2\n",
        "        self.fc7.bias.data = torch.tensor([0.0]) # 1*1\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # Applying ReLU activation after the first layer\n",
        "        x = torch.relu(self.fc2(x))  # layer2 activation\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = torch.relu(self.fc5(x))\n",
        "        x = torch.relu(self.fc6(x))\n",
        "        x = torch.relu(self.fc7(x))\n",
        "        return x\n",
        "\n",
        "# Create an instance of the network\n",
        "net = MLP()\n",
        "\n",
        "# Print the weights and biases of each layer\n",
        "print('Weights and biases of each layer:')\n",
        "print('Weight of the first layer:')\n",
        "print(net.fc1.weight.data.shape)\n",
        "print('Bias of the first layer:')\n",
        "print(net.fc1.bias.data.shape)\n",
        "print('Weight of the second layer:')\n",
        "print(net.fc2.weight.data)\n",
        "print('Bias of the second layer:')\n",
        "print(net.fc2.bias.data)\n",
        "# Print the modified network\n",
        "print('Network structure:')\n",
        "print(net)\n",
        "\n",
        "# Now let's create the input vector x for a batch of 2 inputs\n",
        "x = torch.tensor([[3.0, 4.0,5.0],\n",
        "                  [5.0,4.0, 3.0]])\n",
        "print(x.shape)\n",
        "# Forward pass through the first layer\n",
        "wx_plus_b_fc1 = torch.matmul(x, net.fc1.weight.data.t()) + net.fc1.bias.data\n",
        "print('# Now let\\'s create the input vector x for a batch of 3 inputs')\n",
        "print('x:', x)\n",
        "print('wx + b for the first layer:\\n', wx_plus_b_fc1.t())\n",
        "\n",
        "# Applying ReLU activation to wx + b for the first layer\n",
        "relu_output = torch.relu(wx_plus_b_fc1)\n",
        "print('ReLU applied to wx + b for the first layer:\\n', relu_output.t())\n",
        "\n",
        "# Forward pass through the second layer\n",
        "wx_plus_b_fc2 = torch.matmul(relu_output, net.fc2.weight.data.t()) + net.fc2.bias.data\n",
        "print('wx + b for the second layer:\\n', wx_plus_b_fc2.t())\n",
        "\n",
        "# Applying ReLU activation to wx + b for the second layer\n",
        "relu_output_fc2 = torch.relu(wx_plus_b_fc2)\n",
        "print('ReLU applied to wx + b for the second layer:\\n', relu_output_fc2.t())\n",
        "\n",
        "# Forward pass through the network\n",
        "output = net(x)\n",
        "print('Output after forward pass through the network:\\n', output.t())"
      ]
    }
  ]
}