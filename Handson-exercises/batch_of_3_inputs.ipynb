{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiOPYMVhG11w",
        "outputId": "64a2c26e-85c5-4dd4-e46c-62f0bc886291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check cuda is available or not\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UAgpzuZHQtO",
        "outputId": "c22e3bbd-99be-4a4b-a035-2907cd5f09e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define your network architecture\n",
        "class batch3inputsnetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(batch3inputsnetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(3, 4)  # Input layer to hidden layer\n",
        "        self.fc2 = nn.Linear(4, 2)  # Hidden layer to output layer\n",
        "\n",
        "        # Manually set weights and biases\n",
        "        self.fc1.weight.data = torch.tensor([[1.0, -1.0, 1.0],   # Weights from input to hidden layer\n",
        "                                             [1.0, 1.0, 0.0],\n",
        "                                             [0.0, 1.0, 1.0],\n",
        "                                             [1.0, 0.0, 1.0]])\n",
        "        self.fc1.bias.data = torch.tensor([-5.0, 0.0, 1.0, -2.0])  # Biases for hidden layer\n",
        "\n",
        "        self.fc2.weight.data = torch.tensor([[1.0, 1.0, -1.0, 0.0],\n",
        "                                             [0.0, 0.0, 1.0, -1.0]])  # weights for the layer 2 ( 2*4)\n",
        "        self.fc2.bias.data = torch.tensor([0.0, 1.0])      # bias for output layer (2*1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # Applying ReLU activation after the first layer\n",
        "        x = torch.relu(self.fc2(x))  # Output layer with relu activation\n",
        "        return x\n",
        "\n",
        "# Create an instance of the network\n",
        "net = batch3inputsnetwork()\n",
        "\n",
        "# Print the weights and biases of each layer\n",
        "print('Weights and biases of each layer:')\n",
        "print('Weight of the first layer:')\n",
        "print(net.fc1.weight.data)\n",
        "print('Bias of the first layer:')\n",
        "print(net.fc1.bias.data)\n",
        "print('Weight of the second layer:')\n",
        "print(net.fc2.weight.data)\n",
        "print('Bias of the second layer:')\n",
        "print(net.fc2.bias.data)\n",
        "# Print the modified network\n",
        "print('Network structure:')\n",
        "print(net)\n",
        "\n",
        "# Now let's create the input vector x for a batch of 3 inputs\n",
        "x = torch.tensor([[2.0, 1.0, 3.0],\n",
        "                  [1.0, 1.0, 0.0],\n",
        "                  [0.0, 1.0, 1.0]])\n",
        "\n",
        "# Forward pass through the first layer\n",
        "wx_plus_b_fc1 = torch.matmul(x, net.fc1.weight.data.t()) + net.fc1.bias.data\n",
        "print('# Now let\\'s create the input vector x for a batch of 3 inputs')\n",
        "print('x:', x)\n",
        "print('wx + b for the first layer:\\n', wx_plus_b_fc1.t())\n",
        "\n",
        "# Applying ReLU activation to wx + b for the first layer\n",
        "relu_output = torch.relu(wx_plus_b_fc1)\n",
        "print('ReLU applied to wx + b for the first layer:\\n', relu_output.t())\n",
        "\n",
        "# Forward pass through the second layer\n",
        "wx_plus_b_fc2 = torch.matmul(relu_output, net.fc2.weight.data.t()) + net.fc2.bias.data\n",
        "print('wx + b for the second layer:\\n', wx_plus_b_fc2.t())\n",
        "\n",
        "# Applying ReLU activation to wx + b for the second layer\n",
        "relu_output_fc2 = torch.relu(wx_plus_b_fc2)\n",
        "print('ReLU applied to wx + b for the second layer:\\n', relu_output_fc2.t())\n",
        "\n",
        "# Forward pass through the network\n",
        "output = net(x)\n",
        "print('Output after forward pass through the network:\\n', output.t())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IxqFigkHZTk",
        "outputId": "1038da48-2851-47db-e825-2ffc36f28d4b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights and biases of each layer:\n",
            "Weight of the first layer:\n",
            "tensor([[ 1., -1.,  1.],\n",
            "        [ 1.,  1.,  0.],\n",
            "        [ 0.,  1.,  1.],\n",
            "        [ 1.,  0.,  1.]])\n",
            "Bias of the first layer:\n",
            "tensor([-5.,  0.,  1., -2.])\n",
            "Weight of the second layer:\n",
            "tensor([[ 1.,  1., -1.,  0.],\n",
            "        [ 0.,  0.,  1., -1.]])\n",
            "Bias of the second layer:\n",
            "tensor([0., 1.])\n",
            "Network structure:\n",
            "batch3inputsnetwork(\n",
            "  (fc1): Linear(in_features=3, out_features=4, bias=True)\n",
            "  (fc2): Linear(in_features=4, out_features=2, bias=True)\n",
            ")\n",
            "# Now let's create the input vector x for a batch of 3 inputs\n",
            "x: tensor([[2., 1., 3.],\n",
            "        [1., 1., 0.],\n",
            "        [0., 1., 1.]])\n",
            "wx + b for the first layer:\n",
            " tensor([[-1., -5., -5.],\n",
            "        [ 3.,  2.,  1.],\n",
            "        [ 5.,  2.,  3.],\n",
            "        [ 3., -1., -1.]])\n",
            "ReLU applied to wx + b for the first layer:\n",
            " tensor([[0., 0., 0.],\n",
            "        [3., 2., 1.],\n",
            "        [5., 2., 3.],\n",
            "        [3., 0., 0.]])\n",
            "wx + b for the second layer:\n",
            " tensor([[-2.,  0., -2.],\n",
            "        [ 3.,  3.,  4.]])\n",
            "ReLU applied to wx + b for the second layer:\n",
            " tensor([[0., 0., 0.],\n",
            "        [3., 3., 4.]])\n",
            "Output after forward pass through the network:\n",
            " tensor([[0., 0., 0.],\n",
            "        [3., 3., 4.]], grad_fn=<TBackward0>)\n"
          ]
        }
      ]
    }
  ]
}